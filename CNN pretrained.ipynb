{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataloaders' from '/Users/dov/Library/Mobile Documents/com~apple~CloudDocs/dovsync/Documenti Universita/Advanced Machine Learning/AML Project.nosync/melanoma-detection/dataloaders.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import wandb\n",
    "from config import BATCH_SIZE\n",
    "\n",
    "import dataloaders\n",
    "import utils\n",
    "from importlib import reload\n",
    "import config\n",
    "reload(config)\n",
    "reload(utils)\n",
    "reload(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "INPUT_SIZE = 3\n",
    "NUM_CLASSES = 7\n",
    "HIDDEN_SIZE = [32, 64, 128, 256]\n",
    "N_EPOCHS = 20\n",
    "LR = 1e-3\n",
    "LR_DECAY = 0.85\n",
    "REG = 0.01\n",
    "SEGMENT = True\n",
    "CROP_ROI = True\n",
    "ARCHITECHTURE = \"inception_v3\"\n",
    "DATASET_LIMIT = None\n",
    "DROPOUT_P = 0.1\n",
    "NORMALIZE=True\n",
    "HISTOGRAM_NORMALIZATION = False\n",
    "\n",
    "if CROP_ROI:\n",
    "    assert SEGMENT, f\"Crop roi needs segment to be True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initilization\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42 \n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\")\n",
    "print('Using device: %s'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Data Balance-- balance_data set to True. Training data will be balanced.\n",
      "--Data Balance-- The most common class is 0 with 5364 images.\n",
      "--Data Balance-- The second common class is 4 with 890 images with a difference of 4474 images from the most common class.\n",
      "--Data Balance (Undersampling)-- Removing 1609 from 0 class..\n",
      "--Data Balance (Undersampling)-- 0 now has 1609 images\n",
      "-- Data Balance (Oversampling) -- Adding 1347 from 1 class..\n",
      "-- Data Balance (Oversampling) -- Adding 730 from 2 class..\n",
      "-- Data Balance (Oversampling) -- Adding 1198 from 3 class..\n",
      "-- Data Balance (Oversampling) -- Adding 719 from 4 class..\n",
      "-- Data Balance (Oversampling) -- Adding 1495 from 5 class..\n",
      "-- Data Balance (Oversampling) -- Adding 1517 from 6 class..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 11263it [02:00, 93.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Data Loader--- Images uploaded: 11263\n",
      "Loading complete, some files (0) were not found: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 2003it [00:28, 70.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Data Loader--- Images uploaded: 2003\n",
      "Loading complete, some files (0) were not found: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 1512it [00:12, 116.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Data Loader--- Images uploaded: 1511\n",
      "Loading complete, some files (1) were not found: ['data/HAM10000_images_test/ISIC_0035068.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "resnet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "resnet_std =  torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader, val_loader, test_loader = dataloaders.create_dataloaders(\n",
    "    mean=resnet_mean,\n",
    "    std=resnet_std,\n",
    "    normalize=NORMALIZE,\n",
    "    limit=DATASET_LIMIT,\n",
    "    size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet24Pretrained(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
    "        super(ResNet24Pretrained, self).__init__()\n",
    "        self.model = models.resnet34(pretrained=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=DROPOUT_P),\n",
    "            nn.Linear(self.model.fc.in_features, 256, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Dropout(p=DROPOUT_P),\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Linear(128, num_classes, bias=False),\n",
    "            nn.BatchNorm1d(num_classes),\n",
    "            \n",
    "        )\n",
    "        self.model.fc = self.classifier\n",
    "\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        print(f'Model has {params} trainable params.')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetPretrained(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
    "        super(DenseNetPretrained, self).__init__()\n",
    "        self.model = models.densenet121(pretrained=True)\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=DROPOUT_P),\n",
    "        \n",
    "            nn.Linear(self.model.classifier.in_features, 256, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            nn.Linear(256, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, num_classes, bias=False),\n",
    "            nn.BatchNorm1d(num_classes),\n",
    "        )\n",
    "            \n",
    "        self.model.classifier = self.classifier\n",
    "\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        print(f'Model has {params} trainable params.')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import Inception_V3_Weights\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(InceptionV3, self).__init__()\n",
    "        self.model = models.inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "\n",
    "        print(f\"In features are: {self.model.fc.in_features}\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=DROPOUT_P),\n",
    "        \n",
    "            nn.Linear(self.model.fc.in_features, 1024, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "\n",
    "            nn.Linear(1024, 256, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            nn.Linear(256, 64, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.Linear(64, num_classes, bias=False),\n",
    "            nn.BatchNorm1d(num_classes),\n",
    "        )\n",
    "            \n",
    "        self.model.fc = self.classifier\n",
    "\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        print(f'Model has {params} trainable params.')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mscarcelli-domiziano\u001b[0m (\u001b[33mscarcelli\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/dov/Library/Mobile Documents/com~apple~CloudDocs/dovsync/Documenti Universita/Advanced Machine Learning/AML Project.nosync/melanoma-detection/wandb/run-20231124_200202-fs1iuhgn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/scarcelli/melanoma/runs/fs1iuhgn' target=\"_blank\">glorious-breeze-25</a></strong> to <a href='https://wandb.ai/scarcelli/melanoma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/scarcelli/melanoma' target=\"_blank\">https://wandb.ai/scarcelli/melanoma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/scarcelli/melanoma/runs/fs1iuhgn' target=\"_blank\">https://wandb.ai/scarcelli/melanoma/runs/fs1iuhgn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/scarcelli/melanoma/runs/fs1iuhgn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29c166ec0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a new run\n",
    "wandb.init(\n",
    "    project=\"melanoma\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LR,\n",
    "        \"architecture\": ARCHITECHTURE,\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        'reg': REG,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        \"hidden_size\": HIDDEN_SIZE,\n",
    "        \"dataset\": \"HAM10K\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"segmentation\": SEGMENT,\n",
    "        \"crop_roi\": CROP_ROI,\n",
    "        \"dataset_limit\": DATASET_LIMIT,\n",
    "        \"dropout_p\": DROPOUT_P,\n",
    "        \"normalize\": NORMALIZE,\n",
    "        \"histogram_normalization\": HISTOGRAM_NORMALIZATION\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In features are: 2048\n",
      "Model has 27491094 trainable params.\n",
      "model.fc.1.weight\n",
      "model.fc.3.weight\n",
      "model.fc.3.bias\n",
      "model.fc.4.weight\n",
      "model.fc.6.weight\n",
      "model.fc.6.bias\n",
      "model.fc.7.weight\n",
      "model.fc.9.weight\n",
      "model.fc.9.bias\n",
      "model.fc.10.weight\n",
      "model.fc.11.weight\n",
      "model.fc.11.bias\n"
     ]
    }
   ],
   "source": [
    "if ARCHITECHTURE == \"resnet24\":\n",
    "    model = ResNet24Pretrained(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, norm_layer='BN').to(device)\n",
    "elif ARCHITECHTURE == \"densenet121\":\n",
    "    model = DenseNetPretrained(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, norm_layer='BN').to(device)\n",
    "elif ARCHITECHTURE == \"inception_v3\":\n",
    "    model = InceptionV3(NUM_CLASSES).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown architechture {ARCHITECHTURE}\")\n",
    "\n",
    "# Freezing pretrained CNN backbone for classifier head fine tuning\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "\n",
    "# LAYERS_TO_FINE_TUNE = 20\n",
    "# parameters = list(model.parameters())\n",
    "# for p in parameters[-LAYERS_TO_FINE_TUNE:]:\n",
    "#     p.requires_grad=True\n",
    "    \n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad=True\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=REG)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (images, labels, _) in enumerate(train_loader):\n",
    "#     # Print the size of each image in the batch\n",
    "#     print(f\"Batch {batch_idx + 1}, Image size: {images.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430d9d4b5e63422681ca3c92c69a694e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [1/20], Step [176/176], Loss: 1.2398, Accuracy: 52.7124%\n",
      "Validation -> Validation accuracy for epoch 1 is: 51.3729%\n",
      "Validation -> Validation loss for epoch 1 is: 1.4456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43090dd2f0fd4e259c0e59d766ee6289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [2/20], Step [176/176], Loss: 1.1219, Accuracy: 60.0107%\n",
      "Validation -> Validation accuracy for epoch 2 is: 52.5212%\n",
      "Validation -> Validation loss for epoch 2 is: 1.4205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41edd92261940fd8d17ce6e35de960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [3/20], Step [176/176], Loss: 0.8285, Accuracy: 62.1593%\n",
      "Validation -> Validation accuracy for epoch 3 is: 54.6680%\n",
      "Validation -> Validation loss for epoch 3 is: 1.3975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13ee2cedd14e609fa4496a34d53476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [4/20], Step [176/176], Loss: 1.0276, Accuracy: 64.6187%\n",
      "Validation -> Validation accuracy for epoch 4 is: 54.4683%\n",
      "Validation -> Validation loss for epoch 4 is: 1.4476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4653e614d526418cbed8015a8f16b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [5/20], Step [176/176], Loss: 0.9145, Accuracy: 65.9682%\n",
      "Validation -> Validation accuracy for epoch 5 is: 56.6650%\n",
      "Validation -> Validation loss for epoch 5 is: 1.4235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96b83a9fa347edbd011e467e4f574e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [6/20], Step [176/176], Loss: 1.1320, Accuracy: 67.4421%\n",
      "Validation -> Validation accuracy for epoch 6 is: 55.2671%\n",
      "Validation -> Validation loss for epoch 6 is: 1.4076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cd08c3aadc455d9b9c628cd2515040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [7/20], Step [176/176], Loss: 0.8776, Accuracy: 68.4897%\n",
      "Validation -> Validation accuracy for epoch 7 is: 57.5137%\n",
      "Validation -> Validation loss for epoch 7 is: 1.4081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df60b58807c34219a84336f0c3784897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [8/20], Step [176/176], Loss: 0.9609, Accuracy: 68.8626%\n",
      "Validation -> Validation accuracy for epoch 8 is: 55.2671%\n",
      "Validation -> Validation loss for epoch 8 is: 1.4857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5793339a4b4eea9d0c62fae4b6e33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [9/20], Step [176/176], Loss: 0.7292, Accuracy: 69.6795%\n",
      "Validation -> Validation accuracy for epoch 9 is: 58.6620%\n",
      "Validation -> Validation loss for epoch 9 is: 1.4026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a996b218314e05bfdfb6067540617f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [10/20], Step [176/176], Loss: 0.9468, Accuracy: 70.6828%\n",
      "Validation -> Validation accuracy for epoch 10 is: 53.0704%\n",
      "Validation -> Validation loss for epoch 10 is: 1.5277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b297279bb7ed417f961f9c4fcb8226eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -> Epoch [11/20], Step [176/176], Loss: 0.9174, Accuracy: 71.1977%\n",
      "Validation -> Validation accuracy for epoch 11 is: 56.1158%\n",
      "Validation -> Validation loss for epoch 11 is: 1.4860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e187804588274760a8bf0af6628eb0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dov/Library/Mobile Documents/com~apple~CloudDocs/dovsync/Documenti Universita/Advanced Machine Learning/AML Project.nosync/melanoma-detection/CNN pretrained.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dov/Library/Mobile%20Documents/com~apple~CloudDocs/dovsync/Documenti%20Universita/Advanced%20Machine%20Learning/AML%20Project.nosync/melanoma-detection/CNN%20pretrained.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m CROP_ROI:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dov/Library/Mobile%20Documents/com~apple~CloudDocs/dovsync/Documenti%20Universita/Advanced%20Machine%20Learning/AML%20Project.nosync/melanoma-detection/CNN%20pretrained.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         tr_images \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcrop_roi(tr_images, size\u001b[39m=\u001b[39m(\u001b[39m299\u001b[39m, \u001b[39m299\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dov/Library/Mobile%20Documents/com~apple~CloudDocs/dovsync/Documenti%20Universita/Advanced%20Machine%20Learning/AML%20Project.nosync/melanoma-detection/CNN%20pretrained.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m tr_images \u001b[39m=\u001b[39m tr_images\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dov/Library/Mobile%20Documents/com~apple~CloudDocs/dovsync/Documenti%20Universita/Advanced%20Machine%20Learning/AML%20Project.nosync/melanoma-detection/CNN%20pretrained.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tr_labels \u001b[39m=\u001b[39m tr_labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dov/Library/Mobile%20Documents/com~apple~CloudDocs/dovsync/Documenti%20Universita/Advanced%20Machine%20Learning/AML%20Project.nosync/melanoma-detection/CNN%20pretrained.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tr_outputs \u001b[39m=\u001b[39m model(tr_images)\u001b[39m.\u001b[39mlogits\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "total_step = len(train_loader)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_accuracy = None\n",
    "val_accuracies = []\n",
    "# best_model = type(model)(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, norm_layer='BN') # get a new instance\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    tr_loss_iter = 0\n",
    "    training_count = 0\n",
    "    training_correct_preds = 0\n",
    "    for tr_i, (tr_images, tr_labels, segmentations) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        if SEGMENT:\n",
    "            tr_images = torch.mul(tr_images, segmentations) #Apply segmentation\n",
    "            if CROP_ROI:\n",
    "                tr_images = utils.crop_roi(tr_images, size=(299, 299))\n",
    "        tr_images = tr_images.to(device)\n",
    "        tr_labels = tr_labels.to(device)\n",
    "\n",
    "        tr_outputs = model(tr_images).logits\n",
    "        tr_loss = loss_function(tr_outputs, tr_labels)\n",
    "        wandb.log({\"Training Loss\": tr_loss.item()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tr_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            training_preds = torch.argmax(tr_outputs, -1).detach()\n",
    "            training_count += len(tr_labels)\n",
    "            training_correct_preds += (training_preds == tr_labels).sum()\n",
    "\n",
    "        tr_loss_iter += tr_loss.item()\n",
    "\n",
    "    \n",
    "    current_train_accuracy = 100 * (training_correct_preds/training_count)\n",
    "    wandb.log({\"Training Accuracy\": current_train_accuracy})\n",
    "    print ('Training -> Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.4f}%'\n",
    "            .format(epoch+1, N_EPOCHS, tr_i+1, total_step, tr_loss.item(), current_train_accuracy))\n",
    "            \n",
    "    train_losses.append(tr_loss_iter/(len(train_loader)*BATCH_SIZE))\n",
    "\n",
    "    #LR *= LR_DECAY\n",
    "    #update_lr(optimizer, LR)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_correct_preds = 0\n",
    "        validation_count = 0\n",
    "        val_loss_iter = 0\n",
    "        for val_i, (val_images, val_labels, segmentations) in enumerate(val_loader):\n",
    "            if SEGMENT:\n",
    "                val_images = torch.mul(val_images, segmentations) #Apply segmentation\n",
    "                if CROP_ROI:\n",
    "                    val_images = utils.crop_roi(val_images, size=(299, 299))\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_images)\n",
    "            validation_preds = torch.argmax(val_outputs, -1).detach()   \n",
    "            validation_count += len(val_labels)\n",
    "            validation_correct_preds += (validation_preds == val_labels).sum()\n",
    "            val_loss = loss_function(val_outputs, val_labels)\n",
    "            wandb.log({\"Validation Loss\": val_loss.item()})\n",
    "            val_loss_iter += val_loss.item()\n",
    "        \n",
    "        val_losses.append(val_loss_iter/(len(val_loader)*BATCH_SIZE))\n",
    "\n",
    "        val_accuracy = 100 * (validation_correct_preds / validation_count)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        wandb.log({\"Validation Accuracy\": val_accuracy})\n",
    "        \n",
    "        print('Validation -> Validation accuracy for epoch {} is: {:.4f}%'.format(epoch+1, val_accuracy))\n",
    "        print('Validation -> Validation loss for epoch {} is: {:.4f}'.format(epoch+1, val_loss.item()))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(train_losses, 'r', label='Train loss')\n",
    "plt.plot(val_losses, 'g', label='Val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot([val_accuracy.cpu() for val_accuracy in val_accuracies], 'r', label='Val accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
